Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:  25%|██▌       | 1/4 [00:00<00:00,  8.07it/s]Downloading shards:  50%|█████     | 2/4 [00:00<00:00,  8.06it/s]Downloading shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.01it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00,  8.15it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00,  8.10it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  6.64it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  9.81it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 10.05it/s]
Some weights of the model checkpoint at AI-Sweden-Models/Llama-3-8B-instruct were not used when initializing LlamaEncoderModel: ['lm_head.weight']
- This IS expected if you are initializing LlamaEncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaEncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be removed and `position_embeddings` will be mandatory.
